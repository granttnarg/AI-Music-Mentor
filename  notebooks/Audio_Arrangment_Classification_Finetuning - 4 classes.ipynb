{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA4b3eNnDitQ",
        "outputId": "95e7d0f1-0e76-43f4-f788-2516bf822357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auHV8fWtHwpu",
        "outputId": "5e8ad34c-64dd-4af5-99d2-437cd671c182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.16.1\n",
            "  Downloading tensorflow-2.16.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (3.14.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.16.1)\n",
            "  Downloading ml_dtypes-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (25.0)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.16.1)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (1.74.0)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow==2.16.1)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (3.10.0)\n",
            "Collecting numpy<2.0.0,>=1.26.0 (from tensorflow==2.16.1)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow==2.16.1) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.16.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCCMLCyNH5Z0"
      },
      "source": [
        "## Player setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec6lRHReHw6s"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio, display\n",
        "import os\n",
        "\n",
        "def play_audio(file_path, show_controls=True):\n",
        "    \"\"\"Create an inline audio player in Jupyter\"\"\"\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"üéµ Playing: {os.path.basename(file_path)}\")\n",
        "        display(Audio(file_path, autoplay=False))\n",
        "    else:\n",
        "        print(f\"‚ùå File not found: {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS0PAaSiH9Wf"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import librosa\n",
        "\n",
        "folder_path = Path('/content/drive/MyDrive/<insert_path_here>')\n",
        "\n",
        "track_1 = \"\"\n",
        "track_2 = \"\"\n",
        "track_3 = \"\"\n",
        "\n",
        "full_path_1 = folder_path / track_1\n",
        "full_path_2 = folder_path / track_2\n",
        "full_path_3 = folder_path / track_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tT0YXQRAICxV"
      },
      "outputs": [],
      "source": [
        "play_audio(full_path_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLiGZVhGIJMw"
      },
      "source": [
        "# Remove Silence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22OhEBkOIEmh"
      },
      "outputs": [],
      "source": [
        "from pydub import AudioSegment\n",
        "from pydub.silence import detect_nonsilent\n",
        "from functools import reduce\n",
        "\n",
        "def strip_silence(audio_path):\n",
        "    \"\"\"Removes silent parts from an audio file.\"\"\"\n",
        "    sound = AudioSegment.from_file(audio_path)\n",
        "    nonsilent_ranges = detect_nonsilent(\n",
        "        sound, min_silence_len=500, silence_thresh=-50)\n",
        "    stripped = reduce(lambda acc, val: acc + sound[val[0]:val[1]],\n",
        "                      nonsilent_ranges, AudioSegment.empty())\n",
        "    stripped.export(audio_path, format='mp3')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHd1ia5wIQSq"
      },
      "source": [
        "# Audio Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDTOocg5IK3C"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Audio processing functionality for chorus detection.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "import librosa\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Constants\n",
        "SR = 12000\n",
        "HOP_LENGTH = 128\n",
        "MAX_FRAMES = 300\n",
        "MAX_METERS = 201\n",
        "N_FEATURES = 15\n",
        "\n",
        "\n",
        "class AudioFeature:\n",
        "    \"\"\"Class for extracting and processing audio features.\"\"\"\n",
        "\n",
        "    def __init__(self, audio_path, sr=SR, hop_length=HOP_LENGTH):\n",
        "        self.audio_path = audio_path\n",
        "        self.sr = sr\n",
        "        self.hop_length = hop_length\n",
        "        self.y = None\n",
        "        self.y_harm = self.y_perc = None\n",
        "        self.beats = None\n",
        "        self.chromagram = self.chroma_acts = None\n",
        "        self.combined_features = None\n",
        "        self.key = self.mode = None\n",
        "        self.mel_acts = self.melspectrogram = None\n",
        "        self.meter_grid = None\n",
        "        self.mfccs = self.mfcc_acts = None\n",
        "        self.n_frames = None\n",
        "        self.onset_env = None\n",
        "        self.rms = None\n",
        "        self.spectrogram = None\n",
        "        self.tempo = None\n",
        "        self.tempogram = self.tempogram_acts = None\n",
        "        self.time_signature = 4\n",
        "\n",
        "    def detect_key(self, chroma_vals: np.ndarray) -> Tuple[str, str]:\n",
        "        \"\"\"Detect the key and mode (major or minor) of the audio segment.\"\"\"\n",
        "        note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
        "        major_profile = np.array([6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88])\n",
        "        minor_profile = np.array([6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17])\n",
        "\n",
        "        # Normalize profiles\n",
        "        major_profile /= np.linalg.norm(major_profile)\n",
        "        minor_profile /= np.linalg.norm(minor_profile)\n",
        "\n",
        "        # Calculate correlations for all possible keys\n",
        "        major_correlations = [np.corrcoef(chroma_vals, np.roll(major_profile, i))[0, 1] for i in range(12)]\n",
        "        minor_correlations = [np.corrcoef(chroma_vals, np.roll(minor_profile, i))[0, 1] for i in range(12)]\n",
        "\n",
        "        # Find best match\n",
        "        max_major_idx = np.argmax(major_correlations)\n",
        "        max_minor_idx = np.argmax(minor_correlations)\n",
        "\n",
        "        self.mode = 'major' if major_correlations[max_major_idx] > minor_correlations[max_minor_idx] else 'minor'\n",
        "        self.key = note_names[max_major_idx if self.mode == 'major' else max_minor_idx]\n",
        "        return self.key, self.mode\n",
        "\n",
        "    def calculate_ki_chroma(self, waveform: np.ndarray, sr: int, hop_length: int) -> np.ndarray:\n",
        "        \"\"\"Calculate a normalized, key-invariant chromagram.\"\"\"\n",
        "        chromagram = librosa.feature.chroma_cqt(y=waveform, sr=sr, hop_length=hop_length, bins_per_octave=24)\n",
        "        chromagram = (chromagram - chromagram.min()) / (chromagram.max() - chromagram.min())\n",
        "\n",
        "        chroma_vals = np.sum(chromagram, axis=1)\n",
        "        key, mode = self.detect_key(chroma_vals)\n",
        "\n",
        "        key_idx = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'].index(key)\n",
        "        shift_amount = -key_idx if mode == 'major' else -(key_idx + 3) % 12\n",
        "\n",
        "        return librosa.util.normalize(np.roll(chromagram, shift_amount, axis=0), axis=1)\n",
        "\n",
        "\n",
        "    def extract_features_from_audio(self):\n",
        "        \"\"\"\n",
        "        Extract features from audio file using the same process as training.\n",
        "        \"\"\"\n",
        "        print(f\"Processing: {self.audio_path}\")\n",
        "\n",
        "        # Load audio and separate harmonic/percussive components\n",
        "        self.y, self.sr = librosa.load(self.audio_path, sr=self.sr)\n",
        "        print(f\"Loaded audio: {len(self.y)/self.sr:.1f} seconds\")\n",
        "\n",
        "        self.y_harm, self.y_perc = librosa.effects.hpss(self.y)\n",
        "\n",
        "        # Extract spectrogram and RMS\n",
        "        self.spectrogram, _ = librosa.magphase(librosa.stft(self.y, hop_length=self.hop_length))\n",
        "        self.rms = librosa.feature.rms(S=self.spectrogram, hop_length=self.hop_length).astype(np.float32)\n",
        "\n",
        "        # Extract mel spectrogram and its components\n",
        "        self.melspectrogram = librosa.feature.melspectrogram(\n",
        "            y=self.y, sr=self.sr, n_mels=128, hop_length=self.hop_length).astype(np.float32)\n",
        "        self.mel_acts = librosa.decompose.decompose(\n",
        "            self.melspectrogram, n_components=3, sort=True)[1].astype(np.float32)\n",
        "\n",
        "        # Extract chromagram and its components\n",
        "        self.chromagram = self.calculate_ki_chroma(self.y_harm, self.sr, self.hop_length).astype(np.float32)\n",
        "        self.chroma_acts = librosa.decompose.decompose(\n",
        "            self.chromagram, n_components=4, sort=True)[1].astype(np.float32)\n",
        "\n",
        "        # Extract onset envelope and tempogram\n",
        "        self.onset_env = librosa.onset.onset_strength(y=self.y_perc, sr=self.sr, hop_length=self.hop_length)\n",
        "        self.tempogram = np.clip(librosa.feature.tempogram(\n",
        "            onset_envelope=self.onset_env, sr=self.sr, hop_length=self.hop_length), 0, None)\n",
        "        self.tempogram_acts = librosa.decompose.decompose(self.tempogram, n_components=3, sort=True)[1]\n",
        "\n",
        "        # Extract MFCCs and components\n",
        "        self.mfccs = librosa.feature.mfcc(y=self.y, sr=self.sr, n_mfcc=20, hop_length=self.hop_length)\n",
        "        self.mfccs += abs(np.min(self.mfccs))\n",
        "        self.mfcc_acts = librosa.decompose.decompose(self.mfccs, n_components=4, sort=True)[1].astype(np.float32)\n",
        "\n",
        "        # Combine features with weighted normalization\n",
        "        features = [self.rms, self.mel_acts, self.chroma_acts, self.tempogram_acts, self.mfcc_acts]\n",
        "        feature_names = ['rms', 'mel_acts', 'chroma_acts', 'tempogram_acts', 'mfcc_acts']\n",
        "\n",
        "        # Calculate weights for each feature type\n",
        "        dims = {name: feature.shape[0] for feature, name in zip(features, feature_names)}\n",
        "        total_inv_dim = sum(1 / dim for dim in dims.values())\n",
        "        weights = {name: 1 / (dims[name] * total_inv_dim) for name in feature_names}\n",
        "\n",
        "        # Standardize and weight features\n",
        "        std_weighted_features = [\n",
        "            StandardScaler().fit_transform(feature.T).T * weights[name]\n",
        "            for feature, name in zip(features, feature_names)\n",
        "        ]\n",
        "\n",
        "        self.combined_features = np.concatenate(std_weighted_features, axis=0).T.astype(np.float32)\n",
        "        self.n_frames = len(self.combined_features)\n",
        "\n",
        "    def create_meter_grid(self):\n",
        "        \"\"\"Create a grid based on the meter of the song, using tempo and beats.\"\"\"\n",
        "        self.tempo, self.beats = librosa.beat.beat_track(\n",
        "            onset_envelope=self.onset_env, sr=self.sr, hop_length=self.hop_length)\n",
        "\n",
        "        # Adjust tempo to reasonable range\n",
        "        if self.tempo < 75:\n",
        "            self.tempo *= 2\n",
        "        elif self.tempo > 160:\n",
        "            self.tempo /= 2\n",
        "\n",
        "        self.meter_grid = self._create_meter_grid()\n",
        "        return self.meter_grid\n",
        "\n",
        "    def _create_meter_grid(self) -> np.ndarray:\n",
        "        \"\"\"Helper function to create a meter grid for the song.\"\"\"\n",
        "        seconds_per_beat = 60 / self.tempo\n",
        "        beat_interval = int(librosa.time_to_frames(\n",
        "            seconds_per_beat, sr=self.sr, hop_length=self.hop_length))\n",
        "\n",
        "        # Find best matching start beat\n",
        "        if len(self.beats) >= 3:\n",
        "            best_match = max(\n",
        "                (1 - abs(np.mean(self.beats[i:i+3]) - beat_interval) / beat_interval, self.beats[i])\n",
        "                for i in range(len(self.beats) - 2)\n",
        "            )[1]\n",
        "            anchor_frame = best_match if best_match > 0.95 else self.beats[0]\n",
        "        else:\n",
        "            anchor_frame = self.beats[0] if len(self.beats) > 0 else 0\n",
        "\n",
        "        first_beat_time = librosa.frames_to_time(anchor_frame, sr=self.sr, hop_length=self.hop_length)\n",
        "\n",
        "        # Calculate beat times forward and backward\n",
        "        time_duration = librosa.frames_to_time(self.n_frames, sr=self.sr, hop_length=self.hop_length)\n",
        "        beat_times_forward = np.arange(first_beat_time, time_duration, seconds_per_beat)\n",
        "        beat_times_backward = np.arange(first_beat_time - seconds_per_beat, -seconds_per_beat, -seconds_per_beat)\n",
        "\n",
        "\n",
        "        # Create beat times in both directions\n",
        "        beat_grid = np.concatenate((np.array([0.0]), beat_times_backward[::-1], beat_times_forward))\n",
        "        meter_indices = np.arange(0, len(beat_grid), self.time_signature)\n",
        "        meter_grid = beat_grid[meter_indices]\n",
        "\n",
        "        # Ensure grid starts at 0\n",
        "        if meter_grid[0] != 0.0:\n",
        "            meter_grid = np.insert(meter_grid, 0, 0.0)\n",
        "\n",
        "        # Convert to frames and add final frame\n",
        "        meter_grid_frames = librosa.time_to_frames(meter_grid, sr=self.sr, hop_length=self.hop_length)\n",
        "        meter_grid_frames = np.append(meter_grid_frames, self.n_frames)\n",
        "\n",
        "        return meter_grid_frames\n",
        "\n",
        "\n",
        "def segment_data_meters(data: np.ndarray, meter_grid: List[int]) -> List[np.ndarray]:\n",
        "    \"\"\"Segment input data into chunks based on a meter grid.\"\"\"\n",
        "    return [data[meter_grid[i]:meter_grid[i+1]] for i in range(len(meter_grid) - 1)]\n",
        "\n",
        "\n",
        "def positional_encoding(position: int, d_model: int) -> np.ndarray:\n",
        "    \"\"\"Add positional encoding to input data.\"\"\"\n",
        "    pe = np.zeros(d_model)\n",
        "    for i in range(0, d_model, 2):\n",
        "        pe[i] = np.sin(position / (10000 ** (i / d_model)))\n",
        "        if i + 1 < d_model:\n",
        "            pe[i + 1] = np.cos(position / (10000 ** (i / d_model)))\n",
        "    return pe\n",
        "\n",
        "\n",
        "def apply_hierarchical_positional_encoding(segments: List[np.ndarray]) -> List[np.ndarray]:\n",
        "    \"\"\"Apply positional encoding to a list of segments.\"\"\"\n",
        "    encoded_segments = []\n",
        "    for meter_idx, meter_segment in enumerate(segments):\n",
        "        meter_encoded = np.zeros_like(meter_segment)\n",
        "        for frame_idx, frame in enumerate(meter_segment):\n",
        "            frame_pos_encoding = positional_encoding(frame_idx, frame.shape[0]) * 0.1\n",
        "            meter_pos_encoding = positional_encoding(meter_idx, frame.shape[0]) * 0.2\n",
        "            meter_encoded[frame_idx] = frame + frame_pos_encoding + meter_pos_encoding\n",
        "        encoded_segments.append(meter_encoded)\n",
        "    return encoded_segments\n",
        "\n",
        "\n",
        "def pad_song(encoded_segments: List[np.ndarray], max_frames: int = MAX_FRAMES,\n",
        "             max_meters: int = MAX_METERS, n_features: int = N_FEATURES) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Pad a list of encoded segments to create a uniform 3D array.\n",
        "\n",
        "    Parameters:\n",
        "    - encoded_segments (list): List of encoded data segments\n",
        "    - max_frames (int): Maximum number of frames per segment\n",
        "    - max_meters (int): Maximum number of meters\n",
        "    - n_features (int): Number of features per frame\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: Padded 3D array of shape (max_meters, max_frames, n_features)\n",
        "    \"\"\"\n",
        "    padded_song = np.zeros((max_meters, max_frames, n_features))\n",
        "\n",
        "    for i, segment in enumerate(encoded_segments):\n",
        "        if i >= max_meters:\n",
        "            break  # Only consider up to max_meters segments\n",
        "\n",
        "        segment_frames = segment.shape[0]\n",
        "        if segment_frames <= max_frames:\n",
        "            # If segment fits, copy it directly\n",
        "            padded_song[i, :segment_frames, :] = segment\n",
        "        else:\n",
        "            # If segment is too long, sample frames evenly\n",
        "            indices = np.linspace(0, segment_frames - 1, max_frames, dtype=int)\n",
        "            padded_song[i, :, :] = segment[indices, :]\n",
        "\n",
        "    return padded_song\n",
        "\n",
        "\n",
        "def process_audio(audio_path, trim_silence=True, sr=SR, hop_length=HOP_LENGTH):\n",
        "    \"\"\"Process an audio file for chorus detection.\"\"\"\n",
        "    try:\n",
        "        # Optionally strip silence\n",
        "        if trim_silence:\n",
        "            strip_silence(audio_path)\n",
        "\n",
        "        # Extract audio features\n",
        "        audio_features = AudioFeature(audio_path, sr=sr, hop_length=hop_length)\n",
        "        audio_features.extract_features_from_audio()\n",
        "        meter_grid = audio_features.create_meter_grid()\n",
        "\n",
        "        # Segment and pad the data\n",
        "        feature_segments = segment_data_meters(audio_features.combined_features, meter_grid)\n",
        "        encoded_segments = apply_hierarchical_positional_encoding(feature_segments)\n",
        "        padded_song = pad_song(encoded_segments)\n",
        "\n",
        "        # Add batch dimension for model\n",
        "        padded_song = np.expand_dims(padded_song, axis=0)\n",
        "        return padded_song, audio_features\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing audio: {e}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a7kzbBnIS6f"
      },
      "outputs": [],
      "source": [
        "padded_song, audio_features = process_audio(full_path_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4J6DxUYIUsP"
      },
      "outputs": [],
      "source": [
        "print(padded_song)\n",
        "print(audio_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uNN1RRvJ7c5"
      },
      "source": [
        "# Load CRNN Model and its helper methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfNFMA_fJYsP"
      },
      "outputs": [],
      "source": [
        "crnn_model = \"best_model_V3.h5\"\n",
        "MODEL_PATH = folder_path / crnn_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaDmpZfVKCH3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Model functionality for chorus detection.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import librosa\n",
        "\n",
        "\n",
        "def create_crnn_model(max_frames_per_meter=300, max_meters=201, n_features=15):\n",
        "    \"\"\"\n",
        "    Recreate the exact CRNN model architecture from the repo\n",
        "    \"\"\"\n",
        "    # Frame-level feature extractor (CNN part)\n",
        "    frame_input = tf.keras.layers.Input(shape=(max_frames_per_meter, n_features))\n",
        "    conv1 = tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(frame_input)\n",
        "    pool1 = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(conv1)\n",
        "    conv2 = tf.keras.layers.Conv1D(filters=256, kernel_size=3, activation='relu', padding='same')(pool1)\n",
        "    pool2 = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(conv2)  # Fixed: was pool2, should be conv2\n",
        "    conv3 = tf.keras.layers.Conv1D(filters=256, kernel_size=3, activation='relu', padding='same')(pool2)\n",
        "    pool3 = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(conv3)\n",
        "    frame_features = tf.keras.layers.Flatten()(pool3)\n",
        "    frame_feature_model = tf.keras.Model(inputs=frame_input, outputs=frame_features)\n",
        "\n",
        "    # Full model with LSTM\n",
        "    meter_input = tf.keras.layers.Input(shape=(max_meters, max_frames_per_meter, n_features))\n",
        "    time_distributed = tf.keras.layers.TimeDistributed(frame_feature_model)(meter_input)\n",
        "    masking_layer = tf.keras.layers.Masking(mask_value=0.0)(time_distributed)\n",
        "    lstm_out = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(masking_layer)\n",
        "    output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='sigmoid'))(lstm_out)\n",
        "\n",
        "    model = tf.keras.Model(inputs=meter_input, outputs=output)\n",
        "    return model\n",
        "\n",
        "# Update your load function:\n",
        "def load_CRNN_model(model_path: str = MODEL_PATH) -> tf.keras.Model:\n",
        "    \"\"\"Load a pre-trained CRNN model from the specified path.\"\"\"\n",
        "    try:\n",
        "        # Create the model architecture\n",
        "        model = create_crnn_model()\n",
        "\n",
        "        # Load just the weights\n",
        "        model.load_weights(model_path)\n",
        "        print(\"Model loaded successfully!\")\n",
        "        return model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def smooth_predictions(data: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Apply smoothing to model predictions to reduce jitter.\"\"\"\n",
        "    # First pass: Moving average\n",
        "    window_size = 3\n",
        "    smoothed = np.zeros_like(data)\n",
        "    for i in range(len(data)):\n",
        "        window_start = max(0, i - window_size // 2)\n",
        "        window_end = min(len(data), i + window_size // 2 + 1)\n",
        "        smoothed[i] = np.mean(data[window_start:window_end])\n",
        "\n",
        "    # Second pass: Eliminate short segments\n",
        "    min_segment_length = 2\n",
        "    current_segment_length = 1\n",
        "    current_value = smoothed[0] > 0.5\n",
        "    binary_smoothed = np.zeros_like(smoothed, dtype=int)\n",
        "    binary_smoothed[0] = int(current_value)\n",
        "\n",
        "    for i in range(1, len(smoothed)):\n",
        "        new_value = smoothed[i] > 0.5\n",
        "        if new_value == current_value:\n",
        "            current_segment_length += 1\n",
        "        else:\n",
        "            # If segment is too short, revert to previous value\n",
        "            if current_segment_length < min_segment_length:\n",
        "                for j in range(i - current_segment_length, i):\n",
        "                    binary_smoothed[j] = int(new_value)\n",
        "            current_value = new_value\n",
        "            current_segment_length = 1\n",
        "        binary_smoothed[i] = int(current_value)\n",
        "\n",
        "    # Third pass: Fix final segment if too short\n",
        "    if current_segment_length < min_segment_length:\n",
        "        for j in range(len(smoothed) - current_segment_length, len(smoothed)):\n",
        "            binary_smoothed[j] = int(not current_value)\n",
        "\n",
        "    return binary_smoothed\n",
        "\n",
        "\n",
        "def make_predictions(model, processed_audio, audio_features):\n",
        "    \"\"\"Make chorus predictions using the loaded model.\"\"\"\n",
        "    # Generate predictions\n",
        "    raw_predictions = model.predict(processed_audio).squeeze()\n",
        "\n",
        "    # Limit predictions to actual meters\n",
        "    n_meters = min(len(audio_features.meter_grid) - 1, len(raw_predictions))\n",
        "    predictions = raw_predictions[:n_meters]\n",
        "\n",
        "    # Apply smoothing\n",
        "    smoothed_predictions = smooth_predictions(predictions)\n",
        "\n",
        "    # Calculate time values for display\n",
        "    meter_grid_times = librosa.frames_to_time(\n",
        "        audio_features.meter_grid, sr=audio_features.sr, hop_length=audio_features.hop_length)\n",
        "\n",
        "    # Find chorus segments\n",
        "    chorus_indices = np.where(smoothed_predictions == 1)[0]\n",
        "    chorus_start_times = []\n",
        "    chorus_end_times = []\n",
        "\n",
        "    if len(chorus_indices) > 0:\n",
        "        # Group consecutive indices\n",
        "        groups = []\n",
        "        current_group = [chorus_indices[0]]\n",
        "\n",
        "        for i in range(1, len(chorus_indices)):\n",
        "            if chorus_indices[i] == chorus_indices[i-1] + 1:\n",
        "                current_group.append(chorus_indices[i])\n",
        "            else:\n",
        "                groups.append(current_group)\n",
        "                current_group = [chorus_indices[i]]\n",
        "        groups.append(current_group)\n",
        "\n",
        "        # Display chorus segments\n",
        "        print(\"\\nDetected chorus sections:\")\n",
        "        for i, group in enumerate(groups):\n",
        "            start_time = meter_grid_times[group[0]]\n",
        "            end_time = meter_grid_times[group[-1] + 1]\n",
        "            chorus_start_times.append(start_time)\n",
        "            chorus_end_times.append(end_time)\n",
        "\n",
        "            start_min, start_sec = divmod(start_time, 60)\n",
        "            end_min, end_sec = divmod(end_time, 60)\n",
        "\n",
        "            print(f\"Chorus {i+1}: {int(start_min)}:{start_sec:05.2f} - {int(end_min)}:{end_sec:05.2f}\")\n",
        "    else:\n",
        "        print(\"No choruses detected in this audio file.\")\n",
        "\n",
        "    return smoothed_predictions, chorus_start_times, chorus_end_times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUlm1l32KPH2"
      },
      "outputs": [],
      "source": [
        "print(MODEL_PATH)\n",
        "padded_song, audio_features = process_audio(full_path_2)\n",
        "model = load_CRNN_model()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7Be_Tn9KZJR"
      },
      "source": [
        "# Test predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUxp3OVCKQt8"
      },
      "outputs": [],
      "source": [
        "smoothed_predictions, chorus_start_times, chorus_end_times  = make_predictions(model, padded_song, audio_features)\n",
        "print(smooth_predictions, chorus_start_times, chorus_end_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ontEyaSMpS5"
      },
      "source": [
        "# Change last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCDzJ8yZKxWN"
      },
      "outputs": [],
      "source": [
        "model.summary() # Original Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTdYCKzGMBZX"
      },
      "outputs": [],
      "source": [
        "  def modify_model_for_multiclass(model, class_names):\n",
        "      \"\"\"Modified version that handles custom loss for padding.\"\"\"\n",
        "      num_classes = len(class_names)\n",
        "\n",
        "      # Freeze all layers except the last one\n",
        "      for layer in model.layers:\n",
        "          layer.trainable = False\n",
        "\n",
        "      # Remove the last TimeDistributed Dense layer\n",
        "      x = model.layers[-2].output\n",
        "\n",
        "      # Add dropout before the output layer -> didnt help on small dataset. maybe try it again when we have more training data.\n",
        "      # x = tf.keras.layers.TimeDistributed(\n",
        "      #     tf.keras.layers.Dropout(0.3)\n",
        "      # )(x)\n",
        "\n",
        "      # Add new multiclass output layer\n",
        "      new_output = tf.keras.layers.TimeDistributed(\n",
        "          tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "      )(x)\n",
        "\n",
        "      # Custom loss function that ignores padding\n",
        "      def masked_categorical_crossentropy(y_true, y_pred):\n",
        "          # Create a mask from the true labels (assuming padding is represented by all zeros or -1s)\n",
        "          mask = tf.reduce_sum(tf.cast(tf.not_equal(y_true, -1.0), tf.float32), axis=-1)\n",
        "          mask = tf.cast(tf.not_equal(mask, 0), tf.float32) # Mask is 1 where there is data, 0 where padded\n",
        "\n",
        "          # Calculate categorical crossentropy\n",
        "          cce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "          # Apply mask\n",
        "          masked_cce = cce * mask\n",
        "\n",
        "          # Return average loss (only over non-padded elements)\n",
        "          return tf.reduce_sum(masked_cce) / (tf.reduce_sum(mask) + tf.keras.backend.epsilon()) # Add epsilon for numerical stability\n",
        "\n",
        "      def masked_accuracy(y_true, y_pred):\n",
        "          # Create a mask from the true labels (assuming padding is represented by all zeros or -1s)\n",
        "          mask = tf.reduce_sum(tf.cast(tf.not_equal(y_true, -1.0), tf.float32), axis=-1)\n",
        "          mask = tf.cast(tf.not_equal(mask, 0), tf.float32) # Mask is 1 where there is data, 0 where padded\n",
        "\n",
        "          # Get predictions and true labels (ignoring padding)\n",
        "          y_pred_classes = tf.argmax(y_pred, axis=-1)\n",
        "          y_true_classes = tf.argmax(y_true, axis=-1)\n",
        "\n",
        "          # Apply mask to true and predicted classes\n",
        "          y_true_masked = y_true_classes * tf.cast(mask, tf.int64)\n",
        "          y_pred_masked = y_pred_classes * tf.cast(mask, tf.int64)\n",
        "\n",
        "          # Calculate accuracy only on non-padded elements\n",
        "          correct = tf.cast(tf.equal(y_pred_masked, y_true_masked), tf.float32) * mask\n",
        "\n",
        "          return tf.reduce_sum(correct) / (tf.reduce_sum(mask) + tf.keras.backend.epsilon()) # Add epsilon for numerical stability\n",
        "\n",
        "      # Create and compile new model\n",
        "      new_model = tf.keras.Model(inputs=model.input, outputs=new_output)\n",
        "      new_model.compile(\n",
        "          optimizer='adam',\n",
        "          loss=masked_categorical_crossentropy,\n",
        "          metrics=[masked_accuracy]\n",
        "      )\n",
        "\n",
        "      return new_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9BaAeYHMJQ_"
      },
      "outputs": [],
      "source": [
        "new_model = modify_model_for_multiclass(model, [\"O\", \"A\", \"B\",\"C\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsUd4aPN9sme"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNjQwCaEMPw-"
      },
      "outputs": [],
      "source": [
        "new_model.summary() # New model with 4 classes on output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J94qi2qMsmm"
      },
      "source": [
        "# Setup Data for fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uyu8bBLasJCN"
      },
      "outputs": [],
      "source": [
        "def create_pitch_shifted_audio_files_with_mapping(original_files, pitch_shifts=[-2, -1, 1, 2]):\n",
        "    \"\"\"Create pitch-shifted audio files and return both files and song_id mapping\"\"\"\n",
        "    import librosa\n",
        "    import soundfile as sf\n",
        "\n",
        "    augmented_files = []\n",
        "    song_id_mapping = {}\n",
        "    file_index = 0\n",
        "\n",
        "    # Add original files first\n",
        "    for i, audio_path in enumerate(original_files):\n",
        "        augmented_files.append(audio_path)\n",
        "        song_id_mapping[file_index] = str(i + 1)  # \"1\", \"2\", \"3\", etc.\n",
        "        file_index += 1\n",
        "\n",
        "    # Add pitch-shifted files\n",
        "    for shift in pitch_shifts:\n",
        "        for i, audio_path in enumerate(original_files):\n",
        "            # Create new filename\n",
        "            stem = audio_path.stem\n",
        "            new_name = f\"{stem}_pitch{shift:+d}.wav\"\n",
        "            new_path = audio_path.parent / new_name\n",
        "\n",
        "            # Skip if file already exists\n",
        "            if not new_path.exists():\n",
        "                # Load, pitch shift, and save\n",
        "                y, sr = librosa.load(audio_path, sr=None)\n",
        "                y_shifted = librosa.effects.pitch_shift(y, sr=sr, n_steps=shift)\n",
        "                sf.write(new_path, y_shifted, sr)\n",
        "                print(f\"Created: {new_path}\")\n",
        "            else:\n",
        "                print(f\"Skipping (already exists): {new_path}\")\n",
        "\n",
        "            augmented_files.append(new_path)\n",
        "            song_id_mapping[file_index] = f\"{i + 1}_pitch{shift:+d}\"  # \"1_pitch-2\", etc.\n",
        "            file_index += 1\n",
        "\n",
        "    return augmented_files, song_id_mapping\n",
        "\n",
        "# Then, augment the LABEL data\n",
        "def augment_label_data_with_time_adjustment(original_data, pitch_shifts=[-2, -1, 1, 2]):\n",
        "    augmented_data = original_data.copy()\n",
        "\n",
        "    for shift in pitch_shifts:\n",
        "        time_stretch_ratio = 2 ** (-shift / 12.0)\n",
        "\n",
        "        for entry in original_data:\n",
        "            new_entry = entry.copy()\n",
        "            new_entry['SongID'] = f\"{entry['SongID']}_pitch{shift:+d}\"\n",
        "            new_entry['start_time'] = entry['start_time'] * time_stretch_ratio\n",
        "            new_entry['end_time'] = entry['end_time'] * time_stretch_ratio\n",
        "            augmented_data.append(new_entry)\n",
        "\n",
        "    return augmented_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7aJc1J3bQ0x"
      },
      "outputs": [],
      "source": [
        "def create_time_stretched_audio_files_with_mapping(original_files, stretch_factors=[0.8, 0.9, 1.1, 1.2]):\n",
        "    \"\"\"Create time-stretched audio files (tempo change, no pitch change)\"\"\"\n",
        "    import librosa\n",
        "    import soundfile as sf\n",
        "\n",
        "    augmented_files = []\n",
        "    song_id_mapping = {}\n",
        "    file_index = 0\n",
        "\n",
        "    # Add original files first\n",
        "    for i, audio_path in enumerate(original_files):\n",
        "        augmented_files.append(audio_path)\n",
        "        song_id_mapping[file_index] = str(i + 1)  # \"1\", \"2\", \"3\", etc.\n",
        "        file_index += 1\n",
        "\n",
        "    # Add time-stretched files\n",
        "    for factor in stretch_factors:\n",
        "        for i, audio_path in enumerate(original_files):\n",
        "            # Create new filename\n",
        "            stem = audio_path.stem\n",
        "            new_name = f\"{stem}_tempo{factor:.1f}x.wav\"\n",
        "            new_path = audio_path.parent / new_name\n",
        "\n",
        "            # Skip if file already exists\n",
        "            if not new_path.exists():\n",
        "                # Load, time stretch, and save\n",
        "                y, sr = librosa.load(audio_path, sr=None)\n",
        "                y_stretched = librosa.effects.time_stretch(y, rate=factor)\n",
        "                sf.write(new_path, y_stretched, sr)\n",
        "                print(f\"Created: {new_path}\")\n",
        "            else:\n",
        "                print(f\"Skipping (already exists): {new_path}\")\n",
        "\n",
        "            augmented_files.append(new_path)\n",
        "            song_id_mapping[file_index] = f\"{i + 1}_tempo{factor:.1f}x\"  # \"1_tempo0.8x\", etc.\n",
        "            file_index += 1\n",
        "\n",
        "    return augmented_files, song_id_mapping\n",
        "\n",
        "def augment_label_data_with_tempo_adjustment(original_data, stretch_factors=[0.8, 0.9, 1.1, 1.2]):\n",
        "    \"\"\"Augment label data for tempo-stretched audio\"\"\"\n",
        "    augmented_data = original_data.copy()\n",
        "\n",
        "    for factor in stretch_factors:\n",
        "        # Time stretch ratio is inverse of tempo factor\n",
        "        # factor=0.8 means slower (80% speed) = longer duration (1/0.8 = 1.25x longer)\n",
        "        time_ratio = 1.0 / factor\n",
        "\n",
        "        for entry in original_data:\n",
        "            new_entry = entry.copy()\n",
        "            new_entry['SongID'] = f\"{entry['SongID']}_tempo{factor:.1f}x\"\n",
        "            new_entry['start_time'] = entry['start_time'] * time_ratio\n",
        "            new_entry['end_time'] = entry['end_time'] * time_ratio\n",
        "            augmented_data.append(new_entry)\n",
        "\n",
        "    return augmented_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6AfUKAvMear"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# O is Other\n",
        "# A is low energy\n",
        "# B is high energy\n",
        "# C is Breakdown\n",
        "\n",
        "folder_path = Path('/content/drive/MyDrive/<insert_your_path_here>')\n",
        "\n",
        "class_names = ['O', 'A', 'B', 'C']\n",
        "\n",
        "track_1 = \"\"\n",
        "track_2 = \"\"\n",
        "track_3 = \"\"\n",
        "track_4 = \"\"\n",
        "track_5 = \"\"\n",
        "track_6 = \"\"\n",
        "track_7 = \"\"\n",
        "track_8 = \"\"\n",
        "track_9 = \"\"\n",
        "track_10 = \"\"\n",
        "track_11 = \"\"\n",
        "track_12 = \"\"\n",
        "track_13 = \"\"\n",
        "track_14 = \"\"\n",
        "track_15 = \"\"\n",
        "track_16 = \"\"\n",
        "track_17 = \"\"\n",
        "track_18 = \"\"\n",
        "track_19 = \"\"\n",
        "track_20 = \"\"\n",
        "track_21 = \"\"\n",
        "track_22 = \"\"\n",
        "track_23 = \"\"\n",
        "track_24 = \"\"\n",
        "track_25 = \"\"\n",
        "\n",
        "\n",
        "full_path_1 = folder_path / track_1\n",
        "full_path_2 = folder_path / track_2\n",
        "full_path_3 = folder_path / track_3\n",
        "full_path_4 = folder_path / track_4\n",
        "full_path_5 = folder_path / track_5\n",
        "full_path_6 = folder_path / track_6\n",
        "full_path_7 = folder_path / track_7\n",
        "full_path_8 = folder_path / track_8\n",
        "full_path_9 = folder_path / track_9\n",
        "full_path_10 = folder_path / track_10\n",
        "full_path_11 = folder_path / track_11\n",
        "full_path_12 = folder_path / track_12\n",
        "full_path_13 = folder_path / track_13\n",
        "full_path_14 = folder_path / track_14\n",
        "full_path_15 = folder_path / track_15\n",
        "full_path_16 = folder_path / track_16\n",
        "full_path_17 = folder_path / track_17\n",
        "full_path_18 = folder_path / track_18\n",
        "full_path_19 = folder_path / track_19\n",
        "full_path_20 = folder_path / track_20\n",
        "full_path_21 = folder_path / track_21\n",
        "full_path_22 = folder_path / track_22\n",
        "full_path_23 = folder_path / track_23\n",
        "full_path_24 = folder_path / track_23\n",
        "full_path_25 = folder_path / track_23\n",
        "\n",
        "tracks_for_finetuning = [\n",
        "    full_path_3,\n",
        "    full_path_2,\n",
        "    full_path_1,\n",
        "    full_path_4,\n",
        "    full_path_5,\n",
        "    full_path_6,\n",
        "    full_path_7,\n",
        "    full_path_8,\n",
        "    full_path_9,\n",
        "    full_path_10,\n",
        "    full_path_11,\n",
        "    full_path_12,\n",
        "    full_path_13,\n",
        "    full_path_14,\n",
        "    full_path_15,\n",
        "    full_path_16,\n",
        "    full_path_17,\n",
        "    full_path_18,\n",
        "    full_path_19,\n",
        "    full_path_20,\n",
        "    full_path_21,\n",
        "    full_path_22,\n",
        "    full_path_23,\n",
        "    full_path_24,\n",
        "    full_path_25,\n",
        "]\n",
        "\n",
        "data = [\n",
        "    {'SongID': 1, 'start_time': 0, 'end_time': 57, 'label': 'O'},\n",
        "    {'SongID': 1, 'start_time': 57, 'end_time': 109, 'label': 'B'},\n",
        "    {'SongID': 1, 'start_time': 109, 'end_time': 116, 'label': 'C'},\n",
        "    {'SongID': 1, 'start_time': 116, 'end_time': 131, 'label': 'B'},\n",
        "    {'SongID': 1, 'start_time': 131, 'end_time': 174, 'label': 'A'},\n",
        "    {'SongID': 1, 'start_time': 174, 'end_time': 205, 'label': 'C'},\n",
        "    {'SongID': 1, 'start_time': 205, 'end_time': 241, 'label': 'A'},\n",
        "    {'SongID': 1, 'start_time': 241, 'end_time': 277, 'label': 'A'},\n",
        "    {'SongID': 1, 'start_time': 277, 'end_time': 295, 'label': 'O'},\n",
        "\n",
        "    # ... Add more here\n",
        "\n",
        "]\n",
        "\n",
        "# 1. Create pitch-shifted audio files\n",
        "# augmented_audio_files, song_id_mapping = create_pitch_shifted_audio_files_with_mapping(\n",
        "#     tracks_for_finetuning, [-2, 5]\n",
        "# )\n",
        "# time stretching without pitch shifting\n",
        "augmented_audio_files, song_id_mapping = create_time_stretched_audio_files_with_mapping(\n",
        "    tracks_for_finetuning, [0.8, 0.9, 1.1, 1.2]\n",
        ")\n",
        "print(augmented_audio_files)\n",
        "\n",
        "# # 2. Augment label data\n",
        "# augmented_data = augment_label_data_with_time_adjustment(data, [-2, 5])\n",
        "# print(augmented_data)\n",
        "\n",
        "# for time stretching\n",
        "augmented_data = augment_label_data_with_tempo_adjustment(data, [0.8, 0.9, 1.1, 1.2])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(augmented_data)\n",
        "csv_path = '/content/test_labels_1song.csv'\n",
        "df.to_csv(csv_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRb-0C8NwLz"
      },
      "source": [
        "# Process CSV and extract features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBVWisNgM8fr"
      },
      "outputs": [],
      "source": [
        "def create_multiclass_labels_from_csv(csv_path, song_id, meter_grid_frames, class_names, sr=12000,\n",
        "  hop_length=128):\n",
        "      \"\"\"\n",
        "      Create multiclass labels for a song based on CSV annotations and meter grid.\n",
        "\n",
        "      Parameters:\n",
        "      - csv_path: Path to your labeled CSV\n",
        "      - song_id: ID of the song to process\n",
        "      - meter_grid_frames: Meter grid frames from feature extraction\n",
        "      - class_names: List of class names (e.g., ['verse', 'chorus', 'bridge', 'outro', 'intro'])\n",
        "      - sr: Sample rate\n",
        "      - hop_length: Hop length\n",
        "\n",
        "      Returns:\n",
        "      - aligned_labels: Integer array where each value corresponds to class index\n",
        "      \"\"\"\n",
        "      # Create mapping from string labels to integers\n",
        "      label_to_int = {label: i for i, label in enumerate(class_names)}\n",
        "      print(f\"Label mapping: {label_to_int}\")\n",
        "\n",
        "      # Load CSV and filter for this song\n",
        "      df = pd.read_csv(csv_path)\n",
        "      song_data = df[df['SongID'] == song_id].copy()\n",
        "\n",
        "      if song_data.empty:\n",
        "          print(f\"No data found for song ID {song_id}\")\n",
        "          return None\n",
        "\n",
        "      # Get total frames for this song\n",
        "      total_frames = int(librosa.time_to_frames(\n",
        "          song_data['end_time'].max(), sr=sr, hop_length=hop_length))\n",
        "\n",
        "      # Create label sequence for the entire song (default to first class, e.g., 'verse')\n",
        "      label_sequence = np.zeros(total_frames, dtype=int)\n",
        "\n",
        "      # Fill in labeled sections\n",
        "      for _, row in song_data.iterrows():\n",
        "          start_frame = int(librosa.time_to_frames(row['start_time'], sr=sr, hop_length=hop_length))\n",
        "          end_frame = int(librosa.time_to_frames(row['end_time'], sr=sr, hop_length=hop_length))\n",
        "\n",
        "          if row['label'] in label_to_int:\n",
        "              class_idx = label_to_int[row['label']]\n",
        "              label_sequence[start_frame:end_frame] = class_idx\n",
        "          else:\n",
        "              print(f\"Warning: Unknown label '{row['label']}' - using default class 0\")\n",
        "\n",
        "      # Align labels to meter grid\n",
        "      aligned_labels = []\n",
        "      for i in range(len(meter_grid_frames) - 1):\n",
        "          start_meter = int(meter_grid_frames[i])\n",
        "          end_meter = int(meter_grid_frames[i + 1])\n",
        "\n",
        "          end_meter = min(end_meter, len(label_sequence))\n",
        "\n",
        "          if start_meter < len(label_sequence):\n",
        "              # Use majority vote for each meter\n",
        "              meter_section = label_sequence[start_meter:end_meter]\n",
        "              if len(meter_section) > 0:\n",
        "                  # Get most frequent label in this meter\n",
        "                  most_common_label = np.bincount(meter_section).argmax()\n",
        "                  aligned_labels.append(most_common_label)\n",
        "              else:\n",
        "                  aligned_labels.append(0)  # Default to class 0\n",
        "          else:\n",
        "              aligned_labels.append(0)\n",
        "\n",
        "      return np.array(aligned_labels)\n",
        "\n",
        "def process_song_multiclass(audio_path, song_id, csv_path, class_names):\n",
        "    \"\"\"Process song for multiclass classification.\"\"\"\n",
        "    # Extract features using the AudioFeature class\n",
        "    audio_features = AudioFeature(audio_path)\n",
        "    audio_features.extract_features_from_audio()\n",
        "    meter_grid = audio_features.create_meter_grid()\n",
        "\n",
        "    # Segment and encode the data\n",
        "    feature_segments = segment_data_meters(audio_features.combined_features, meter_grid)\n",
        "    encoded_segments = apply_hierarchical_positional_encoding(feature_segments)\n",
        "\n",
        "\n",
        "    # Create multiclass labels\n",
        "    labels = create_multiclass_labels_from_csv(csv_path, song_id, meter_grid, class_names)\n",
        "\n",
        "    if labels is None:\n",
        "        return None, None\n",
        "\n",
        "    return encoded_segments, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeIeaTRTNXGR"
      },
      "outputs": [],
      "source": [
        "def prepare_multiclass_data(encoded_segments_list, labels_list, class_names, max_frames=500, max_meters=201):\n",
        "    \"\"\"Prepare data for multiclass training with one-hot encoding.\"\"\"\n",
        "    def pad_segments(segments, max_frames, max_meters, n_features=15):\n",
        "        \"\"\"Pad segments to fixed dimensions.\"\"\"\n",
        "        padded = np.zeros((max_meters, max_frames, n_features))\n",
        "        for i, segment in enumerate(segments):\n",
        "            if i >= max_meters:\n",
        "                break\n",
        "            segment_frames = segment.shape[0]\n",
        "            if segment_frames <= max_frames:\n",
        "                padded[i, :segment_frames, :] = segment\n",
        "            else:\n",
        "                # Sample frames if segment is too long\n",
        "                indices = np.linspace(0, segment_frames - 1, max_frames, dtype=int)\n",
        "                padded[i, :, :] = segment[indices, :]\n",
        "        return padded\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "    for segments, labels in zip(encoded_segments_list, labels_list):\n",
        "        padded_song = pad_segments(segments, max_frames, max_meters)\n",
        "        X.append(padded_song)\n",
        "\n",
        "        # Truncate labels if longer than max_meters before padding\n",
        "        truncated_labels = labels[:max_meters]\n",
        "\n",
        "        # One-hot encode the labels and pad with a class of -1 for padding\n",
        "        padded_labels = np.full((max_meters, num_classes), -1.0, dtype=np.float32) # Use -1.0 for padding\n",
        "        if len(truncated_labels) > 0:\n",
        "            one_hot_labels = tf.keras.utils.to_categorical(truncated_labels, num_classes=num_classes)\n",
        "            padded_labels[:len(truncated_labels), :] = one_hot_labels\n",
        "\n",
        "        y.append(padded_labels)\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZM8DhSYPDHB"
      },
      "source": [
        "# Setup our X and y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuI2c7qJOabq"
      },
      "outputs": [],
      "source": [
        "all_segments = []\n",
        "all_labels = []\n",
        "\n",
        "for i, audio_file in enumerate(augmented_audio_files):\n",
        "    song_id = song_id_mapping[i]\n",
        "    segments, labels = process_song_multiclass(audio_file, song_id, csv_path, class_names)\n",
        "    if segments is not None:\n",
        "        all_segments.append(segments)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "# Prepare data\n",
        "# X, y = prepare_multiclass_data(all_segments, all_labels, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRTLSNcnOpwF"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of songs processed: {len(all_segments)}\")\n",
        "print(f\"Number of labels processed: {len(all_labels)}\")\n",
        "# print(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1cQX_VDP3f6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data at the song level\n",
        "train_segments, val_segments, train_labels, val_labels = train_test_split(\n",
        "    all_segments, all_labels, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train songs: {len(train_segments)}, Validation songs: {len(val_segments)}\")\n",
        "\n",
        "# Prepare (pad) the training and validation data separately\n",
        "print(\"Padding training data...\")\n",
        "X_train, y_train = prepare_multiclass_data(\n",
        "    train_segments, train_labels, class_names, max_frames=MAX_FRAMES, max_meters=MAX_METERS\n",
        ")\n",
        "print(f\"Padded Training shapes: X={X_train.shape}, y={y_train.shape}\")\n",
        "\n",
        "print(\"Padding validation data...\")\n",
        "X_val, y_val = prepare_multiclass_data(\n",
        "    val_segments, val_labels, class_names, max_frames=MAX_FRAMES, max_meters=MAX_METERS\n",
        ")\n",
        "print(f\"Padded Validation shapes: X={X_val.shape}, y={y_val.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBAoQTCjP_7I"
      },
      "outputs": [],
      "source": [
        "# def log_training_run(model, X_train, y_train, X_val, y_val, epochs, batch_size, comment=\"\"):\n",
        "#     \"\"\"\n",
        "#     Trains the model and logs the training history with a comment to a CSV file.\n",
        "#     \"\"\"\n",
        "#     print(f\"Starting training run: {comment}\")\n",
        "#     history = model.fit(\n",
        "#         X_train, y_train,\n",
        "#         epochs=epochs,\n",
        "#         batch_size=batch_size,\n",
        "#         verbose=1,\n",
        "#         validation_data=(X_val, y_val)\n",
        "#     )\n",
        "\n",
        "#     print(\"\\nHistory keys after training:\")\n",
        "#     print(history.history.keys()) # Print keys to debug\n",
        "\n",
        "#     # Prepare log entry\n",
        "#     log_entry = {\n",
        "#         'timestamp': pd.Timestamp.now(),\n",
        "#         'comment': comment,\n",
        "#         'epochs': epochs,\n",
        "#         'batch_size': batch_size,\n",
        "#         'train_loss': history.history['loss'][-1],\n",
        "#         'train_accuracy': history.history['masked_accuracy'][-1], # Use the correct key names\n",
        "#         'val_loss': history.history['val_loss'][-1],\n",
        "#         'val_accuracy': history.history['val_masked_accuracy'][-1] # Use the correct key names\n",
        "#     }\n",
        "\n",
        "#     # Create DataFrame and save to CSV\n",
        "#     log_df = pd.DataFrame([log_entry])\n",
        "\n",
        "#     # Append to CSV, create file with header if it doesn't exist\n",
        "#     if not os.path.exists(log_file_path):\n",
        "#         log_df.to_csv(log_file_path, index=False, header=True)\n",
        "#     else:\n",
        "#         log_df.to_csv(log_file_path, index=False, header=False, mode='a')\n",
        "\n",
        "\n",
        "#     print(f\"Training run logged to {log_file_path}\")\n",
        "\n",
        "#     return history\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def log_training_run(model, X_train, y_train, X_val, y_val, epochs, batch_size,\n",
        "                     comment=\"\", class_weights=None):\n",
        "    \"\"\"\n",
        "    Trains the model and logs the training history with a comment to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    - class_weights: Dict with class weights {0: weight, 1: weight, ...} or None for no weighting\n",
        "    \"\"\"\n",
        "\n",
        "    # Add class weights info to comment if provided\n",
        "    if class_weights is not None:\n",
        "        weights_str = \", \".join([f\"{k}:{v}\" for k, v in class_weights.items()])\n",
        "        comment = f\"{comment} [weights: {weights_str}]\"\n",
        "\n",
        "    print(f\"Starting training run: {comment}\")\n",
        "\n",
        "    # Build fit parameters\n",
        "    fit_params = {\n",
        "        'x': X_train,\n",
        "        'y': y_train,\n",
        "        'epochs': epochs,\n",
        "        'batch_size': batch_size,\n",
        "        'verbose': 1,\n",
        "        'validation_data': (X_val, y_val)\n",
        "    }\n",
        "\n",
        "    # Add class weights if provided\n",
        "    if class_weights is not None:\n",
        "        fit_params['class_weight'] = class_weights\n",
        "        print(f\"Using class weights: {class_weights}\")\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(**fit_params)\n",
        "\n",
        "    print(\"\\nHistory keys after training:\")\n",
        "    print(history.history.keys()) # Print keys to debug\n",
        "\n",
        "    # Prepare log entry\n",
        "    log_entry = {\n",
        "        'timestamp': pd.Timestamp.now(),\n",
        "        'comment': comment,\n",
        "        'epochs': epochs,\n",
        "        'batch_size': batch_size,\n",
        "        'class_weights': str(class_weights) if class_weights else 'None',\n",
        "        'train_loss': history.history['loss'][-1],\n",
        "        'train_accuracy': history.history['masked_accuracy'][-1], # Use the correct key names\n",
        "        'val_loss': history.history['val_loss'][-1],\n",
        "        'val_accuracy': history.history['val_masked_accuracy'][-1] # Use the correct key names\n",
        "    }\n",
        "\n",
        "    # Create DataFrame and save to CSV\n",
        "    log_df = pd.DataFrame([log_entry])\n",
        "\n",
        "    # Append to CSV, create file with header if it doesn't exist\n",
        "    if not os.path.exists(log_file_path):\n",
        "        log_df.to_csv(log_file_path, index=False, header=True)\n",
        "    else:\n",
        "        log_df.to_csv(log_file_path, index=False, header=False, mode='a')\n",
        "\n",
        "    print(f\"Training run logged to {log_file_path}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "# Predefined class weight strategies for your use case\n",
        "CLASS_WEIGHT_STRATEGIES = {\n",
        "  'boost_A_strategy' = {\n",
        "      0: 1.5,   # O (Intro/Outro) - boost recall (40.7% is low)\n",
        "      1: 2.0,   # A (Medium Energy) - boost recall (19.6% is terrible)\n",
        "      2: 0.7,   # B (High Energy) - REDUCE to improve precision\n",
        "      3: 1.0    # C (Breakdown) - normal (decent performance)\n",
        "  }\n",
        "\n",
        "}\n",
        "\n",
        "# Usage examples:\n",
        "print(\"Available class weight strategies:\")\n",
        "for strategy_name, weights in CLASS_WEIGHT_STRATEGIES.items():\n",
        "    print(f\"\\n{strategy_name}:\")\n",
        "    class_names = ['O(Other)', 'A(High)', 'B(Breakdown)', 'C(Low)']\n",
        "    for class_idx, weight in weights.items():\n",
        "        print(f\"  {class_names[class_idx]}: {weight}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"USAGE EXAMPLES:\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uoVp7IZQ3T0"
      },
      "outputs": [],
      "source": [
        "# from datetime import datetime\n",
        "# date = datetime.now()\n",
        "log_file_path = f'/content/training_logs_17-8-25.csv'\n",
        "# log_training_run(new_model, X_train, y_train, X_val, y_val, 15, 1, comment=\"25 songs labelled + time stretching *4 - 4 classes - finetuned\")\n",
        "\n",
        "# Try more aggressive approach\n",
        "log_training_run(\n",
        "    new_model, X_train, y_train, X_val, y_val, 15, 1,\n",
        "    comment=\"25 songs + time stretch *4 - aggressive high energy\",\n",
        "    class_weights=CLASS_WEIGHT_STRATEGIES['boost_A_strategy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MlOg156mK3P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q09wB59M3Sc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate_per_song_and_overall(model, X_val, y_val, class_names):\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Overall metrics (all songs combined)\n",
        "    overall_true, overall_pred = [], []\n",
        "\n",
        "    # Per-song metrics\n",
        "    for song_idx in range(X_val.shape[0]):\n",
        "        song_true, song_pred = [], []\n",
        "\n",
        "        for time_idx in range(y_val.shape[1]):\n",
        "            if np.sum(y_val[song_idx, time_idx]) > 0:\n",
        "                true_class = np.argmax(y_val[song_idx, time_idx])\n",
        "                pred_class = np.argmax(y_pred[song_idx, time_idx])\n",
        "\n",
        "                song_true.append(true_class)\n",
        "                song_pred.append(pred_class)\n",
        "                overall_true.append(true_class)\n",
        "                overall_pred.append(pred_class)\n",
        "\n",
        "        print(f\"Song {song_idx + 1} accuracy: {accuracy_score(song_true, song_pred):.3f}\")\n",
        "\n",
        "    # Overall report\n",
        "    return classification_report(overall_true, overall_pred, target_names=class_names, output_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfOWh-TwOZzD"
      },
      "source": [
        "# look at percision and recall for our classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGucRnMiM9WL"
      },
      "outputs": [],
      "source": [
        "# Then add this evaluation step\n",
        "print(\"\\nPer-class performance analysis:\")\n",
        "class_report = evaluate_per_song_and_overall(new_model, X_val, y_val, class_names)\n",
        "\n",
        "for class_name in class_names:\n",
        "    metrics = class_report[class_name]\n",
        "    print(f\"{class_name}: Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f}, F1={metrics['f1-score']:.3f}\")\n",
        "\n",
        "# percision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLmGp1VRSgn-"
      },
      "source": [
        "# Make new prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr3SDl7Aqmwn"
      },
      "outputs": [],
      "source": [
        "def predict_song(audio_path, model, class_names):\n",
        "    \"\"\"\n",
        "    Predict arrangement sections for a single song using new_model.\n",
        "\n",
        "    Parameters:\n",
        "    - audio_path: str, path to the audio file\n",
        "    - model: tf.keras.Model, trained model (e.g. new_model)\n",
        "    - class_names: list of str, class labels used in training\n",
        "\n",
        "    Returns:\n",
        "    - predicted_classes: NumPy array of predicted class indices per meter\n",
        "    - meter_grid: NumPy array of meter grid frames\n",
        "    \"\"\"\n",
        "    # Preprocess the song (same as training pipeline)\n",
        "    padded_song, audio_features = process_audio(audio_path)\n",
        "    if padded_song is None:\n",
        "        print(\"‚ùå Failed to process song\")\n",
        "        return None, None\n",
        "\n",
        "    # Run the model\n",
        "    preds = model.predict(padded_song).squeeze(axis=0)  # shape: (meters, classes)\n",
        "\n",
        "    # Convert to class indices\n",
        "    predicted_classes = np.argmax(preds, axis=-1)\n",
        "\n",
        "    return predicted_classes, audio_features.meter_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4I6ocQVu-ZG"
      },
      "outputs": [],
      "source": [
        "class_names = [ \"O\", \"A\", \"B\", \"C\"]  # same as you used in new_model\n",
        "\n",
        "new_path = Path('/content/drive/MyDrive/<insert your path here>')\n",
        "\n",
        "new_song = \"\"\n",
        "\n",
        "full_path_wip = new_path / new_song\n",
        "\n",
        "\n",
        "pred_labels, meter_grid = predict_song(full_path_wip, new_model, class_names)\n",
        "\n",
        "print(pred_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocxFjbpzwzE1"
      },
      "source": [
        "# Visualize Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRPYkAK7vGO2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Visualization utilities for multiclass energy level detection.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "def plot_meter_lines(ax: plt.Axes, meter_grid_times: np.ndarray) -> None:\n",
        "    \"\"\"Draw meter grid lines on the plot.\"\"\"\n",
        "    for time in meter_grid_times:\n",
        "        ax.axvline(x=time, color='grey', linestyle='--', linewidth=1, alpha=0.6)\n",
        "\n",
        "\n",
        "def plot_multiclass_predictions(audio_features, multiclass_predictions, class_names,\n",
        "                                title=None, save_path=None):\n",
        "    \"\"\"\n",
        "    Plot the audio waveform and overlay the predicted energy level classifications.\n",
        "\n",
        "    Parameters:\n",
        "    - audio_features: AudioFeature object containing audio data\n",
        "    - multiclass_predictions: Array of class predictions (0=other, 1=high, 2=low, 3=breakdown)\n",
        "    - class_names: List of class names (e.g., ['other', 'high', 'low', 'breakdown'])\n",
        "    - title: Optional title for the plot (default: based on audio filename)\n",
        "    - save_path: Optional path to save the plot image (default: don't save)\n",
        "\n",
        "    Returns:\n",
        "    - fig: The matplotlib figure object\n",
        "    \"\"\"\n",
        "\n",
        "    plt.close('all')  # Add this line\n",
        "    meter_grid_times = librosa.frames_to_time(\n",
        "        audio_features.meter_grid, sr=audio_features.sr, hop_length=audio_features.hop_length)\n",
        "    fig, ax = plt.subplots(figsize=(15, 4), dpi=96)\n",
        "    ax.clear()\n",
        "\n",
        "    meter_grid_times = librosa.frames_to_time(\n",
        "        audio_features.meter_grid, sr=audio_features.sr, hop_length=audio_features.hop_length)\n",
        "    fig, ax = plt.subplots(figsize=(15, 4), dpi=96)\n",
        "\n",
        "    # Display waveform components\n",
        "    librosa.display.waveshow(audio_features.y_harm, sr=audio_features.sr,\n",
        "                              alpha=0.8, ax=ax, color='deepskyblue', label='Harmonic')\n",
        "    librosa.display.waveshow(audio_features.y_perc, sr=audio_features.sr,\n",
        "                              alpha=0.7, ax=ax, color='plum', label='Percussive')\n",
        "    plot_meter_lines(ax, meter_grid_times)\n",
        "\n",
        "    # Define colors for each class\n",
        "    class_colors = {\n",
        "        0: 'lightgray',    # other\n",
        "        1: 'red',          # high energy\n",
        "        2: 'blue',         # low energy\n",
        "        3: 'orange'        # breakdown\n",
        "    }\n",
        "\n",
        "    # Track which classes we've added to legend\n",
        "    legend_added = {class_idx: False for class_idx in range(len(class_names))}\n",
        "\n",
        "    # Highlight sections by energy level\n",
        "    for i, prediction in enumerate(multiclass_predictions):\n",
        "        if i < len(meter_grid_times) - 1:\n",
        "            start_time = meter_grid_times[i]\n",
        "            end_time = meter_grid_times[i + 1]\n",
        "\n",
        "            color = class_colors.get(prediction, 'black')\n",
        "            class_name = class_names[prediction] if prediction < len(class_names) else 'unknown'\n",
        "\n",
        "            # Add to legend only once per class\n",
        "            label = class_name.title() if not legend_added[prediction] else None\n",
        "            legend_added[prediction] = True\n",
        "\n",
        "            ax.axvspan(start_time, end_time, color=color, alpha=0.4, label=label)\n",
        "\n",
        "    # Configure plot appearance\n",
        "    ax.set_xlim([0, len(audio_features.y) / audio_features.sr])\n",
        "    ax.set_ylabel('Amplitude')\n",
        "\n",
        "    # Set plot title\n",
        "    if title:\n",
        "        ax.set_title(title)\n",
        "    else:\n",
        "        audio_file_name = os.path.basename(audio_features.audio_path)\n",
        "        ax.set_title(f'Energy Level Predictions for {os.path.splitext(audio_file_name)[0]}')\n",
        "\n",
        "    # Add legend\n",
        "    ax.legend(loc='upper right')\n",
        "\n",
        "    # Set time-based x-axis labels\n",
        "    duration = len(audio_features.y) / audio_features.sr\n",
        "    xticks = np.arange(0, duration, 30)  # Every 30 seconds for electronic music\n",
        "    xlabels = [f\"{int(tick // 60)}:{int(tick % 60):02d}\" for tick in xticks]\n",
        "    ax.set_xticks(xticks)\n",
        "    ax.set_xticklabels(xlabels)\n",
        "    ax.set_xlabel('Time (mm:ss)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save if path is provided\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_energy_timeline(audio_features, multiclass_predictions, class_names,\n",
        "                        title=None, save_path=None):\n",
        "    \"\"\"\n",
        "    Plot energy levels as a colored timeline/bar chart.\n",
        "\n",
        "    Parameters:\n",
        "    - audio_features: AudioFeature object containing audio data\n",
        "    - multiclass_predictions: Array of class predictions\n",
        "    - class_names: List of class names\n",
        "    - title: Optional title for the plot\n",
        "    - save_path: Optional path to save the plot image\n",
        "\n",
        "    Returns:\n",
        "    - fig: The matplotlib figure object\n",
        "    \"\"\"\n",
        "    duration = len(audio_features.y) / audio_features.sr\n",
        "    meter_grid_times = librosa.frames_to_time(\n",
        "        audio_features.meter_grid, sr=audio_features.sr, hop_length=audio_features.hop_length)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(15, 3), dpi=96)\n",
        "\n",
        "    # Define colors and y-positions for each class\n",
        "    class_colors = {\n",
        "        0: 'lightgray',    # other\n",
        "        1: 'red',          # high energy\n",
        "        2: 'blue',         # low energy\n",
        "        3: 'orange'        # breakdown\n",
        "    }\n",
        "\n",
        "    class_y_positions = {\n",
        "        0: 0,    # other (bottom)\n",
        "        1: 3,    # high energy (top)\n",
        "        2: 1,    # low energy\n",
        "        3: 2     # breakdown (middle-high)\n",
        "    }\n",
        "\n",
        "    # Plot bars for each meter\n",
        "    for i, prediction in enumerate(multiclass_predictions):\n",
        "        if i < len(meter_grid_times) - 1:\n",
        "            start_time = meter_grid_times[i]\n",
        "            end_time = meter_grid_times[i + 1]\n",
        "            width = end_time - start_time\n",
        "\n",
        "            color = class_colors.get(prediction, 'black')\n",
        "            y_pos = class_y_positions.get(prediction, 0)\n",
        "\n",
        "            ax.barh(y_pos, width, left=start_time, height=0.8,\n",
        "                    color=color, alpha=0.8, edgecolor='white', linewidth=0.5)\n",
        "\n",
        "    # Configure plot appearance\n",
        "    ax.set_xlim([0, duration])\n",
        "    ax.set_ylim([-0.5, 3.5])\n",
        "    ax.set_yticks(list(class_y_positions.values()))\n",
        "    ax.set_yticklabels([class_names[i].title() for i in sorted(class_y_positions.keys())])\n",
        "\n",
        "    # Set plot title\n",
        "    if title:\n",
        "        ax.set_title(title)\n",
        "    else:\n",
        "        audio_file_name = os.path.basename(audio_features.audio_path)\n",
        "        ax.set_title(f'Energy Level Timeline for {os.path.splitext(audio_file_name)[0]}')\n",
        "\n",
        "    # Set time-based x-axis labels\n",
        "    xticks = np.arange(0, duration, 30)\n",
        "    xlabels = [f\"{int(tick // 60)}:{int(tick % 60):02d}\" for tick in xticks]\n",
        "    ax.set_xticks(xticks)\n",
        "    ax.set_xticklabels(xlabels)\n",
        "    ax.set_xlabel('Time (mm:ss)')\n",
        "    ax.set_ylabel('Energy Level')\n",
        "\n",
        "    # Add grid\n",
        "    ax.grid(True, axis='x', alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save if path is provided\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_energy_distribution(multiclass_predictions, class_names, title=None):\n",
        "    \"\"\"\n",
        "    Plot a pie chart showing the distribution of energy levels.\n",
        "\n",
        "    Parameters:\n",
        "    - multiclass_predictions: Array of class predictions\n",
        "    - class_names: List of class names\n",
        "    - title: Optional title for the plot\n",
        "\n",
        "    Returns:\n",
        "    - fig: The matplotlib figure object\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Count occurrences of each class\n",
        "    unique, counts = np.unique(multiclass_predictions, return_counts=True)\n",
        "\n",
        "    # Colors matching the timeline\n",
        "    colors = ['lightgray', 'red', 'blue', 'orange']\n",
        "\n",
        "    # Pie chart\n",
        "    ax1.pie(counts, labels=[class_names[i].title() for i in unique],\n",
        "            colors=[colors[i] for i in unique], autopct='%1.1f%%', startangle=90)\n",
        "    ax1.set_title('Energy Level Distribution')\n",
        "\n",
        "    # Bar chart\n",
        "    ax2.bar([class_names[i].title() for i in unique], counts,\n",
        "            color=[colors[i] for i in unique], alpha=0.8)\n",
        "    ax2.set_title('Energy Level Counts')\n",
        "    ax2.set_ylabel('Number of Meters')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    if title:\n",
        "        fig.suptitle(title)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "# Usage example:\n",
        "def visualize_predictions_complete(audio_features, predicted_classes, class_names):\n",
        "    \"\"\"\n",
        "    Create a complete visualization suite for energy level predictions.\n",
        "    \"\"\"\n",
        "    # Only use actual meters (not padded)\n",
        "    actual_meters = min(len(audio_features.meter_grid) - 1, len(predicted_classes))\n",
        "    predicted_classes = predicted_classes[:actual_meters]\n",
        "\n",
        "    print(f\"Creating visualizations for {actual_meters} meters...\")\n",
        "\n",
        "    # Plot 1: Waveform with energy level overlay\n",
        "    plot_multiclass_predictions(audio_features, predicted_classes, class_names)\n",
        "\n",
        "    # Plot 2: Energy timeline\n",
        "    plot_energy_timeline(audio_features, predicted_classes, class_names)\n",
        "\n",
        "    # Plot 3: Distribution charts\n",
        "    plot_energy_distribution(predicted_classes, class_names)\n",
        "\n",
        "    return predicted_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHKmo9XQxHAt"
      },
      "outputs": [],
      "source": [
        "_, audio_features = process_audio(full_path_wip)\n",
        "\n",
        "visualize_predictions_complete(audio_features, pred_labels, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m25sU6FwxqBp"
      },
      "outputs": [],
      "source": [
        "\n",
        "new_song_2 = \"\"\n",
        "\n",
        "full_path_wip_2 = new_path / new_song_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8eVnpfKOXhv"
      },
      "outputs": [],
      "source": [
        "\n",
        "new_song_2 = \"\"\n",
        "\n",
        "full_path_wip_2 = new_path / new_song_2\n",
        "\n",
        "pred_labels, meter_grid = predict_song(full_path_wip_2, new_model, class_names)\n",
        "\n",
        "_, audio_features = process_audio(full_path_wip_2)\n",
        "\n",
        "visualize_predictions_complete(audio_features, pred_labels, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OhWYcL_vMAg"
      },
      "outputs": [],
      "source": [
        "\n",
        "new_song_3 = \"\"\n",
        "\n",
        "full_path_wip_3 = new_path / new_song_3\n",
        "\n",
        "print(full_path_wip_3)\n",
        "\n",
        "pred_labels, meter_grid = predict_song(str(full_path_wip_3), new_model, class_names)\n",
        "\n",
        "_, audio_features = process_audio(full_path_wip_3)\n",
        "\n",
        "visualize_predictions_complete(audio_features, pred_labels, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT6W8m6oU38f"
      },
      "outputs": [],
      "source": [
        "play_audio(full_path_wip_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXQxxgBcyyee"
      },
      "source": [
        "# Pickle dataset - boilerplate that needs adjusting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kpQ8I5vxzXF"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "# import glob\n",
        "# import os\n",
        "\n",
        "# # --- Placeholder for your custom audio processing functions ---\n",
        "# # It's assumed these functions are defined elsewhere and work correctly.\n",
        "# # For this script to be runnable, you must provide their actual implementations.\n",
        "\n",
        "# SR = 22050  # Example Sample Rate\n",
        "# HOP_LENGTH = 512 # Example Hop Length\n",
        "\n",
        "# def strip_silence(audio_path):\n",
        "#     \"\"\"Placeholder: Implement your silence trimming logic here.\"\"\"\n",
        "#     # print(f\"Stripping silence from {audio_path}...\")\n",
        "#     pass\n",
        "\n",
        "# class AudioFeature:\n",
        "#     \"\"\"Placeholder: Implement your AudioFeature class.\"\"\"\n",
        "#     def __init__(self, audio_path, sr, hop_length):\n",
        "#         self.path = audio_path\n",
        "#         self.sr = sr\n",
        "#         self.hop_length = hop_length\n",
        "#         self.combined_features = np.random.rand(128, 500).astype(np.float32) # Example shape\n",
        "\n",
        "#     def extract_features_from_audio(self):\n",
        "#         # print(\"Extracting features...\")\n",
        "#         pass\n",
        "\n",
        "#     def create_meter_grid(self):\n",
        "#         # print(\"Creating meter grid...\")\n",
        "#         return np.random.rand(500).astype(np.float32) # Example shape\n",
        "\n",
        "# def segment_data_meters(features, grid):\n",
        "#     \"\"\"Placeholder: Implement your segmentation logic.\"\"\"\n",
        "#     # print(\"Segmenting data...\")\n",
        "#     return np.random.rand(10, 128, 50).astype(np.float32) # Example shape\n",
        "\n",
        "# def apply_hierarchical_positional_encoding(segments):\n",
        "#     \"\"\"Placeholder: Implement your encoding logic.\"\"\"\n",
        "#     # print(\"Applying encoding...\")\n",
        "#     return segments + np.random.rand(*segments.shape).astype(np.float32)\n",
        "\n",
        "# def pad_song(encoded_segments):\n",
        "#     \"\"\"Placeholder: Implement your padding logic.\"\"\"\n",
        "#     # print(\"Padding song...\")\n",
        "#     # Example: pad to a fixed number of segments, e.g., 12\n",
        "#     padded = np.zeros((12, 128, 50), dtype=np.float32)\n",
        "#     num_segments = min(12, encoded_segments.shape[0])\n",
        "#     padded[:num_segments, :, :] = encoded_segments[:num_segments, :, :]\n",
        "#     return padded\n",
        "\n",
        "# # --- End of Placeholders ---\n",
        "\n",
        "\n",
        "# def process_audio(audio_path, trim_silence=True, sr=SR, hop_length=HOP_LENGTH):\n",
        "#     \"\"\"\n",
        "#     Processes a single audio file using your custom pipeline.\n",
        "#     This function now takes a byte string path from tf.py_function.\n",
        "#     \"\"\"\n",
        "#     # Decode the byte string path to a regular Python string\n",
        "#     audio_path = audio_path.decode('utf-8')\n",
        "\n",
        "#     try:\n",
        "#         # 1. Optionally strip silence\n",
        "#         if trim_silence:\n",
        "#             strip_silence(audio_path)\n",
        "\n",
        "#         # 2. Extract audio features\n",
        "#         audio_features = AudioFeature(audio_path, sr=sr, hop_length=hop_length)\n",
        "#         audio_features.extract_features_from_audio()\n",
        "#         meter_grid = audio_features.create_meter_grid()\n",
        "\n",
        "#         # 3. Segment and pad the data\n",
        "#         feature_segments = segment_data_meters(audio_features.combined_features, meter_grid)\n",
        "#         encoded_segments = apply_hierarchical_positional_encoding(feature_segments)\n",
        "#         padded_song = pad_song(encoded_segments)\n",
        "\n",
        "#         # Note: The original returned np.expand_dims(padded_song, axis=0)\n",
        "#         # We remove the batch dimension here because the tf.data pipeline will add it later.\n",
        "#         return padded_song\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error processing audio at path {audio_path}: {e}\")\n",
        "#         # Return a zero-array or handle error appropriately for your model\n",
        "#         # The shape must match the expected output shape.\n",
        "#         return np.zeros((12, 128, 50), dtype=np.float32)\n",
        "\n",
        "\n",
        "# def load_and_process_item(file_path):\n",
        "#     \"\"\"\n",
        "#     A wrapper function to load audio and extract the label.\n",
        "#     This function will be mapped over the dataset of file paths.\n",
        "#     \"\"\"\n",
        "#     # 1. Extract Label from filename\n",
        "#     # The file_path is a tf.string tensor, so we use TensorFlow string operations\n",
        "#     parts = tf.strings.split(file_path, os.path.sep)\n",
        "#     filename = parts[-1]\n",
        "#     label_str = tf.strings.split(filename, \"_\")[-1]\n",
        "#     label_str = tf.strings.split(label_str, \".\")[0] # Assumes .wav or similar\n",
        "#     label = tf.strings.to_number(label_str, out_type=tf.int32)\n",
        "\n",
        "#     # 2. Process audio using your custom Python function\n",
        "#     # We use tf.py_function to wrap the Python code.\n",
        "#     # We must define the return type (Tout) for TensorFlow to build the graph.\n",
        "#     processed_audio = tf.py_function(\n",
        "#         func=process_audio,\n",
        "#         inp=[file_path],\n",
        "#         Tout=tf.float32\n",
        "#     )\n",
        "\n",
        "#     # Set the shape of the output tensor, which is required after tf.py_function\n",
        "#     # Update this shape to match the exact output of your `pad_song` function.\n",
        "#     processed_audio.set_shape([12, 128, 50])\n",
        "\n",
        "#     return processed_audio, label\n",
        "\n",
        "\n",
        "# def create_preprocessed_dataset(audio_dir, output_path, file_extension=\"*.wav\"):\n",
        "#     \"\"\"\n",
        "#     Creates and saves a preprocessed TensorFlow dataset.\n",
        "\n",
        "#     Args:\n",
        "#         audio_dir (str): Path to the folder containing audio files.\n",
        "#         output_path (str): Path to save the processed tf.data.Dataset.\n",
        "#         file_extension (str): The file extension pattern to search for (e.g., \"*.wav\", \"*.mp3\").\n",
        "#     \"\"\"\n",
        "#     print(f\"üîç Searching for audio files in: {audio_dir}\")\n",
        "#     file_paths = glob.glob(os.path.join(audio_dir, file_extension))\n",
        "\n",
        "#     if not file_paths:\n",
        "#         print(\"‚ö†Ô∏è No audio files found. Please check the `audio_dir` and `file_extension`.\")\n",
        "#         return\n",
        "\n",
        "#     print(f\"‚úÖ Found {len(file_paths)} files. Creating dataset...\")\n",
        "\n",
        "#     # 1. Create a dataset from the list of file paths\n",
        "#     path_ds = tf.data.Dataset.from_tensor_slices(file_paths)\n",
        "\n",
        "#     # 2. Use .map() to apply the processing function to each file\n",
        "#     # `num_parallel_calls=tf.data.AUTOTUNE` processes multiple files in parallel for speed.\n",
        "#     audio_ds = path_ds.map(load_and_process_item, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "#     # 3. Save the processed dataset to disk\n",
        "#     print(f\"üíæ Saving dataset to: {output_path}\")\n",
        "#     # The new, stable API for saving datasets\n",
        "#     audio_ds.save(output_path)\n",
        "#     print(\"‚ú® Dataset creation complete!\")\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # --- Configuration ---\n",
        "#     # Define the path to your folder with raw audio files\n",
        "#     SAMPLES_FOLDER = full_path_wip_2\n",
        "#     # Define where you want to save the processed dataset\n",
        "#     OUTPUT_DATASET_PATH = '/content/drive/MyDrive/DSR-AI-MENTOR'\n",
        "\n",
        "#     # Create the dataset\n",
        "#     create_preprocessed_dataset(SAMPLES_FOLDER, OUTPUT_DATASET_PATH, file_extension=\"*.wav\")\n",
        "\n",
        "#     # --- Example of how to load and use the dataset ---\n",
        "#     print(\"\\n--- Verifying the saved dataset ---\")\n",
        "#     if os.path.exists(OUTPUT_DATASET_PATH):\n",
        "#         loaded_dataset = tf.data.Dataset.load(OUTPUT_DATASET_PATH)\n",
        "\n",
        "#         # Take one sample to inspect its shape and type\n",
        "#         for audio_data, label in loaded_dataset.take(1):\n",
        "#             print(f\"Sample audio data shape: {audio_data.shape}\")\n",
        "#             print(f\"Sample audio data type: {audio_data.dtype}\")\n",
        "#             print(f\"Sample label: {label.numpy()}\")\n",
        "#             print(f\"Sample label type: {label.dtype}\")\n",
        "\n",
        "#         # You can now use this loaded_dataset for training\n",
        "#         # For example:\n",
        "#         # loaded_dataset = loaded_dataset.shuffle(1024).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "#         # model.fit(loaded_dataset, epochs=10)\n",
        "#     else:\n",
        "#         print(f\"Could not find the saved dataset at {OUTPUT_DATASET_PATH}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h83stTGYx0dB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
